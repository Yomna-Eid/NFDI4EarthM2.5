# Overview of data cube technologies and review of other emerging technologies

This document describes the state of the art solutions for local and cloud based data storage and analysis for large gridded data.
In this document we are going to describe the software that is currently used for the analysis of geospatial raster data. 
This is going to be a living document that is updated at least once a year and other emerging technologies are going to be added in the future.
Geospatial raster data is used in very different user scenarios which can be used as points of view of the comparison of the different raster analysis tools. 
There is the distinction between the used computing resources.
The computing resources can go from a single laptop, to a local cluster where the processing needs to be multi threaded or distributed
 up to cloud computing environments which are composed of multiple clusters.
On their own laptop, scientists are able to control their computing environment but they would have to install software themselves. 
The software for laptops is mainly doing computations single threaded and there is a higher level of interactivity. 
In High Performance Computing (HPC) environments the software runs mainly multi-threaded or multi-core and the software is installed by a local system administrator. 
The computation environment is suited for larger processing and is not as interactive as on a local computer, because computation resources need to be mitigated by a job scheduler. 
In a cloud computing environment like the Google Earth Engine (GEE) or the European Open Science Cloud (EOSC) users are bound to what the cloud provider allows on the platform and this can be quite closed like the GEE where only a special version of Python or Javascript is usable or open as on the EOSC where the user can use containers to run any software on the provided computing resources. 
The tools that are described here can be used for different tasks. 
It can go from extraction of a data for a given spatial and temporal extent to the visualisation of the data, to the processing of the data in batches which also includes machine learning training. These different tasks have all different software needs. 


- scope of the document
- disclaimer, that other technologies are coming later
- short overview of use scenarios, from single scientist on Laptop over HPC to Cloud computing
- from data extraction visualization to huge batch processing or ML training
- size of the used datasets local to regional to global

- one possible way to categorize is type of tool and cloud-readiness


## Data formats and databases

In this section we discuss data formats and data bases that are often used for the handling of large gridded geospatial data. 

### Cloud Optimized Geotiffs
Cloud optimized geotiffs (COG) are geotiff files which are organized, so that they can be hosted on a HTTP file server. 
This allows more efficient workflows on the cloud. 
COGs in contrast to plain geotiffs can be opened only partially and so it is not needed to download the whole file but you can access only the parts of the file, that are actually needed. 
The cloud optimized geotiff files can be used like normal tiff files and 
therefore this format is supported by many libraries and software solutions. 


### HDF5

### NetCDF

NetCDF stands for Network Common Data Form and is one of the standard data formats in the geosciences. It is a binary format. The development for NetCDF started in 1988 by UCAR. A NetCDF file is self describing, which means, that it includes a header that describes the data that is included in the netcdf file. 
The most common used NetCDF version is NetCDF-4 which is based on the [HDF 5](#hdf5) format. 
It allows for efficient subsetting but it can not be accessed multi threaded.


### TileDB 
TileDB is an open source universal storage engine for dense and sparse multidimensional arrays. In May 2017 TileDB spun out of Intel Labs and the MIT. TileDB allows parallel I/O and supports both local as well as remote storage systems (i.e. object stores, HDFS and Lustre). Data is stored in tiles/chunks. TileDB provides filters like compression and byte shuffling that can be applied on the chunk/tile level. Instead of wirting the data in place, TileDB creates a new fragment with each write operations. While this idea of fragments improves latency for write operations this introduces overhead for read operations.

### Zarr

Zarr is a data format for handling of large N-dimensional typed arrays. It focuses on support for distributed storage systems (i.e. object stores). 
It aims to provide efficient I/O for parallel computing.
It can handle different chunks and there are mutliple extensions of the data format for different use cases. It can handle compression. 
It is for data sets that are larger than RAM. The computations on these datasets should be parallizable.



## Open Source Software solutions

### EarthDataLab.jl

### Rasters.jl

### Rasdaman (DLR ?)

Rasdaman is an open source Array / Datacube Database System, which pioneered the whole field. They offer an SQL-like query language called rasql for data definition, retrieval and manipulation. Rasql embeds into standard SQL, sets and implements the ISO 9075 SQL Part 15 for Multi-Dimensional Arrays. It is set up as a client server system.  
Datasets have to be imported to the server with rasql. There are different client implementations from Rasdaman: a command-line utility, a python and web client. All of them offer access to the server through rasql. Other third-party clients for different languages and use cases are available as well.

### OpenEO (Uni M端nster ?)

### XArray
Xarray is a Python package, that provides an array type with labels and dimension names on top of a NumPy array. Dimensions, coordinates and attributes gives a more intuitive, more concise and less error-prone developer experience.  Xarray allows to apply operations along dimension names, select values of the array based on the label and not only on the integer positions. The mathematical operations are broadcasted across multiple dimensions not based on the array shape, but based on the array labels. 
You can keep track of metadata as a python dictionary. 
You do not need to keep track of the order of the arrays and you do not need to align dimensions with added dimensions of length one.

### XCube


### Iris

Iris is a python package for the analysis and visualisation of Earth science data. Its data model is based on the [[CF Conventions]]. The visualisation is based on [[matplotlib]] and [[cartopy]]. 
The data formats that can be used with Iris are:
[[NetCDF]], [[GRIB]] and [[PP]]. It also has a plugin system available to include other formats.
The Iris package builds upon [[numpy]] and [[dask]].  
It interoperates with the wider python ecosystem by using the standard numpy dask array types as underlying data storage. 

### Open Data Cube

The Open Data Cube (ODC) is a Python based geospatial data management and analysis software.
The ODC can be used to catalogue large amounts of data and to provide them as a Python API for analysis. 
It is mainly used for the analysis of earth observation data in the framework of regional or national data cube platforms.

### stars (Uni M端nster?)

### terra


## Cloud solutions

### Google Earth Engine

### DIAS

### EOSC


### OpenEO (Uni M端nster ?)


## Mischellaneous Technologies

### Spatio Temporal Asset Catalog (STAC) (Uni M端nster ?)

### Interplanetary File System

### DataLad
